{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35d7f29a-e1a6-4ab7-8968-519d7f6429c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the job titles separated by commas (e.g., Data Scientist, Software Engineer):  Data Scientist\n",
      "Enter the locations separated by commas (e.g., New York, California):  New York, Dallas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping jobs for 'Data Scientist' in 'New York'...\n",
      "pages to scrape: 5\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Reached the limit of pages to scrape.\n",
      "Scraping jobs for 'Data Scientist' in 'Dallas'...\n",
      "pages to scrape: 5\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Reached the limit of pages to scrape.\n",
      "All job details saved to simplyhired_all_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def scrape_simplyhired_jobs(job_title=\"Data Scientist\", location=\"United States\", radius=10, days_ago=30):\n",
    "    \n",
    "    # Set up Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Search URL with filters for radius, days ago, and sorting by date\n",
    "    base_url = \"https://www.simplyhired.com/search\"\n",
    "    search_url = f\"{base_url}?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+')}&s=d&sr=50&t=30\"\n",
    "    \n",
    "    # filter for 10 miles radius and jobs posted in the last 30 days\n",
    "    search_url += f\"&radius={radius}&posted=30\"\n",
    "    \n",
    "    # Adding sorting by date to sort the job listings by latest first\n",
    "    search_url += \"&sort=date\"\n",
    "\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)  \n",
    "\n",
    "    # Initialize result lists\n",
    "    job_qualifications = []\n",
    "    job_descriptions = []\n",
    "    job_links = []\n",
    "\n",
    "    # Find the total number of pages safely\n",
    "    try:\n",
    "        # Try to find pagination elements \n",
    "        pagination_elements = driver.find_elements(By.XPATH, \"//a[@aria-label='Page number']\")\n",
    "        if pagination_elements:\n",
    "            total_pages = int(pagination_elements[-1].text)  # Get the last page number\n",
    "            print(f\"Total pages available: {total_pages}\")\n",
    "        else:\n",
    "            total_pages = 5  # If pagination elements aren't found, assume there's only 5 page\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding total pages: {e}\")\n",
    "        total_pages = 5  # Default to 5 page if there's an issue\n",
    "\n",
    "    pages_to_scrape = total_pages\n",
    "    print(\"pages to scrape:\", pages_to_scrape)\n",
    "\n",
    "    # Pagination loop \n",
    "    current_page = 1\n",
    "    while current_page <= pages_to_scrape:\n",
    "        print(f\"Scraping page {current_page}...\")\n",
    "        \n",
    "        try:\n",
    "            # Find all jobs on the current page\n",
    "            job_elements = driver.find_elements(By.XPATH, \"//*[@id='job-list']/li\")\n",
    "            for i in range(1, len(job_elements) + 1):\n",
    "                try:\n",
    "                    # Select the job\n",
    "                    job_xpath = f\"//*[@id='job-list']/li[{i}]\"\n",
    "                    job_element = driver.find_element(By.XPATH, job_xpath)\n",
    "                    job_element.click()\n",
    "                    time.sleep(3)  # Allow the job details to load\n",
    "\n",
    "                    # Extract job details\n",
    "                    try:\n",
    "                        qualification_elements = driver.find_elements(By.XPATH, \"//ul[@class='chakra-wrap__list css-19lo6pj']/li/span[@data-testid='viewJobQualificationItem']\")\n",
    "                        qualifications = [qual.text for qual in qualification_elements]\n",
    "                    except:\n",
    "                        qualifications = []\n",
    "\n",
    "                    try:\n",
    "                        description = driver.find_element(By.XPATH, \"//div[@data-testid='viewJobBodyJobFullDescriptionContent']\").text\n",
    "                    except:\n",
    "                        description = \"No description available\"\n",
    "\n",
    "                    try:\n",
    "                        link = driver.current_url\n",
    "                    except:\n",
    "                        link = \"No link available\"\n",
    "\n",
    "                    # Append details to lists\n",
    "                    job_qualifications.append(qualifications)\n",
    "                    job_descriptions.append(description)\n",
    "                    job_links.append(link)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing job {i}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Move to the next page if applicable\n",
    "            if current_page < pages_to_scrape:\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.XPATH, \"//a[contains(@aria-label, 'Next')]\")\n",
    "                    next_button.click()\n",
    "                    time.sleep(5)\n",
    "                    current_page += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to next page: {e}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Reached the limit of pages to scrape.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page}: {e}\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame({\n",
    "        \"Job Description\": job_descriptions,\n",
    "        \"Qualifications\": [\"; \".join(qual) for qual in job_qualifications],\n",
    "        \"Job Link\": job_links\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Multiple job roles and locations input\n",
    "    job_titles_input = input(\"Enter the job titles separated by commas (e.g., Data Scientist, Software Engineer): \").strip().split(',')\n",
    "    location_input = input(\"Enter the locations separated by commas (e.g., New York, California): \").strip().split(',')\n",
    "\n",
    "    job_titles_input = [title.strip() for title in job_titles_input]\n",
    "    location_input = [loc.strip() for loc in location_input]\n",
    "\n",
    "    # Iterate over each combination of job role and location\n",
    "    all_job_data = []\n",
    "    for job_title in job_titles_input:\n",
    "        for location in location_input:\n",
    "            print(f\"Scraping jobs for '{job_title}' in '{location}'...\")\n",
    "            df = scrape_simplyhired_jobs(job_title=job_title, location=location)\n",
    "            all_job_data.append(df)\n",
    "\n",
    "    # Combine all results into one DataFrame\n",
    "    final_df = pd.concat(all_job_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    final_df.to_csv(\"simplyhired_jobs.csv\", index=False)\n",
    "    print(\"All job details saved to simplyhired_all_jobs.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0b490-1cfc-49be-b977-eae8cfc98be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
